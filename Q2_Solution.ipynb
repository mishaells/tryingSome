{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Human Activity Recognition Using WiFi Signals\n",
        "\n",
        "## Overview\n",
        "Human Activity Recognition (HAR) using WiFi signals leverages the unique properties of wireless channel variations to detect different activities.\n",
        "\n",
        "## Data Format\n",
        "- **WiFi signal data** is similar to image data in structure, represented in the shape `(channels, height, width)`, but with a different interpretation:\n",
        "  - `channels` → **channel**\n",
        "  - `height` → **Time Steps**\n",
        "  - `width` → **Antenna Pairs (transmitter-receiver combinations)**\n",
        "- **Labels** represent a predefined set of classes, as is typical in classification tasks."
      ],
      "metadata": {
        "id": "0NnOoFtyHilj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Data"
      ],
      "metadata": {
        "id": "1GqPqJNJIWNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"alihabibullah/question-2-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnmdeTv2xPQ2",
        "outputId": "77422f97-850b-46f8-9cb4-7d4c9f835e6a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/alihabibullah/question-2-data?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204M/204M [00:01<00:00, 145MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/alihabibullah/question-2-data/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8hne08gxqjN",
        "outputId": "42516299-0985-4060-8851-c1d0937c03b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['WiFiSensingDataset.pt']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oEZvoIOGxqWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DwIkG1iHVPg",
        "outputId": "220aae8a-a93d-4ff0-bd96-9f95f9a5b248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-4d2815cbd2c7>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(f\"{path}/WiFiSensingDataset.pt\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Load the .pt file\n",
        "data = torch.load(f\"{path}/WiFiSensingDataset.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Analyze the Dataset ( Stored in `data`)\n",
        "\n",
        "1. **Determine the number of unique labels** in the dataset.  \n",
        "\n",
        "2. **Determine the shape of the input data** (number of samples and features).  \n",
        "\n",
        "3. **Find the maximum value** in the dataset.  \n",
        "\n",
        "4. **Find the minimum value** in the dataset.  "
      ],
      "metadata": {
        "id": "8vkoyStvJBLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Determine the number of unique labels in y_train\n",
        "number_of_labels = len(torch.unique(data['y_train']))\n",
        "print(f\"Number of unique labels: {number_of_labels}\")\n",
        "\n",
        "# 2. Determine the shape of the input data (X_train)\n",
        "data_shape = data['X_train'].shape\n",
        "print(f\"Shape of the input data (X_train): {data_shape} (n_samples, channels, time_steps, subcarriers)\")\n",
        "\n",
        "# 3. Find the maximum value in the dataset (X_train)\n",
        "maximum_value = torch.max(data['X_train'])\n",
        "print(f\"Maximum value in X_train: {maximum_value}\")\n",
        "\n",
        "# 4. Find the minimum value in the dataset (X_train)\n",
        "minimum_value = torch.min(data['X_train'])\n",
        "print(f\"Minimum value in X_train: {minimum_value}\")"
      ],
      "metadata": {
        "id": "7lPNBr_UK1nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03728d9e-4869-4bc1-c741-ae24269978e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique labels: 7\n",
            "Shape of the input data (X_train): torch.Size([2500, 1, 250, 90]) (n_samples, channels, time_steps, subcarriers)\n",
            "Maximum value in X_train: 1.0\n",
            "Minimum value in X_train: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Build and Evaluate a Neural Network\n",
        "\n",
        "1. **Design a Neural Network (Maximum 5 Layers)**  \n",
        "   Build a compact neural network with no more than 5 layers. Clearly specify the type of each layer (e.g., Dense) and any activation functions used.\n",
        "\n",
        "2. **Evaluate Your Model**  \n",
        "   Train your network on the provided dataset and report the evaluation metrics (e.g., accuracy, loss). Discuss the performance of your model and any challenges faced during training.\n"
      ],
      "metadata": {
        "id": "zbbW9TqJJI84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Build and Evaluate a Neural Network\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create a simple neural network (maximum 5 layers)\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Prepare the dataset\n",
        "X_train = data['X_train'].reshape(data['X_train'].shape[0], -1)  # Flatten for fully connected layers\n",
        "y_train = data['y_train'].long()\n",
        "X_test = data['X_test'].reshape(data['X_test'].shape[0], -1)\n",
        "y_test = data['y_test'].long()\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the model\n",
        "input_size = X_train.shape[1]\n",
        "num_classes = number_of_labels\n",
        "model = SimpleNN(input_size, num_classes)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    predictions = torch.argmax(test_outputs, axis=1)\n",
        "    accuracy = (predictions == y_test).sum().item() / y_test.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "hci0PeriJIGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5952e74-77dc-43e3-a8ae-71eca2b8c8eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 2.5785\n",
            "Epoch 2/10, Loss: 0.8548\n",
            "Epoch 3/10, Loss: 0.8127\n",
            "Epoch 4/10, Loss: 0.9009\n",
            "Epoch 5/10, Loss: 1.4779\n",
            "Epoch 6/10, Loss: 1.3959\n",
            "Epoch 7/10, Loss: 1.8243\n",
            "Epoch 8/10, Loss: 0.6906\n",
            "Epoch 9/10, Loss: 3.1787\n",
            "Epoch 10/10, Loss: 0.4917\n",
            "Test Accuracy: 42.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good luck in the exam x)\n",
        "\n",
        "Prepared by: Ahmed Y. Radwan\n"
      ],
      "metadata": {
        "id": "6pqoK28DLPB4"
      }
    }
  ]
}